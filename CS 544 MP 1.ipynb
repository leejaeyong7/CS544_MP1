{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 544 MP 1\n",
    "This Notebook serves as a simple baseline for MP1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "This document assumes that you can run jupyter notebook with python 3, with packages:\n",
    "1. Numpy\n",
    "2. Matplotlib\n",
    "\n",
    "Installed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Problem Statement\n",
    "I have been asked when a Quasi-Newton method implemented using conjugate gradient is better than Polak Ribiere, or vice versa. I should very much like to know a better answer than the one I gave (that I always use Quasi-Newton first). Using some problems from your domain, and in groups no larger than four, produce an experimental answer to this question. You should compare small problems (10's-100's of variables) and large problems (1000's or more variables). You should compare cases where you can expect large third derivatives to cases where the Hessian shouldn't change much. Does the restart strategy in Polak-Ribiere make a difference?\n",
    "\n",
    "Note that I am encouraging clever experimentation here, rather than blank coding efficiency. You should be able to get good implementations of each method from various locations, and it's fine to use those. Appropriate comparisons are the number of steps, the speed of convergence, the resources required, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cameras.json File Format \n",
    "### Top level\n",
    "Top level consists of simple dictionary of camera index to camera objects.\n",
    "\n",
    "Camera Index is a unique integer that refers to camera, from 0 to 10\n",
    "\n",
    "For camera objects, see camera objects section for further details.\n",
    "```\n",
    "{\n",
    "  CameraId: Camera Object // see Camera Object\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "### Camera Object\n",
    "Camera object contains multiple components.\n",
    "\n",
    "R: Rotation matrix in extrinsic matrix\n",
    "\n",
    "t: translation vector in extrinsic matrix\n",
    "\n",
    "E: 3x4 extrinsic matrix\n",
    "\n",
    "K: 3x3 intrinsic matrix\n",
    "\n",
    "`K.dot(E)` would yield projection matrix (3x4)\n",
    "\n",
    "```\n",
    "{\n",
    "  R: Rotation Matrix,// (3x3 list),\n",
    "  t: translation, //(3x1 list),\n",
    "  E: extrinsic Matrix //(3x4 list),\n",
    "  K: intrinsic Matrix //(3x3 list),\n",
    "  W: image width\n",
    "  H: image height\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matches.json File Format \n",
    "### Top level\n",
    "Top level consists of simple array of Feature objects\n",
    "\n",
    "See Feature objects section for further details.\n",
    "\n",
    "```\n",
    "[\n",
    "  Feature Object // see Feature Object\n",
    "]\n",
    "```\n",
    "\n",
    "### Feature Object\n",
    "Feature object is a dictionary of camera_id to feature point of image.\n",
    "\n",
    "feature point of image has x and y:\n",
    "\n",
    "x: coordinate of feature in normalized image coordinate\n",
    "\n",
    "y: coordinate of feature in normalized image coordinate\n",
    "\n",
    "normalized image coordinage simply means that it is center-subtracted.\n",
    "\n",
    "For instance, if x = 0, y = 100 in image of size W=1000, H=600,\n",
    "\n",
    "the feature coordinage would be at 500 ( because 1000 / 2 - 0), 200 ( because 600 / 2 - 100)\n",
    "\n",
    "CameraId corresponds to integer index of the camera.\n",
    "One example feature object would look like:\n",
    "```json\n",
    "{\n",
    "  \"0\": {\"x\" : 100, \"y\":101.1},\n",
    "  \"1\": {\"x\" : 10, \"y\": -50},\n",
    "  \"5\": {\"x\" : 12, \"y\": 82}\n",
    "}\n",
    "```\n",
    "above example shows that for this feature point, we have 3 images that contains this feature point, namely image 0, image 1, and image 5.\n",
    "In image 0, the feature point is at w/2 + 100, and h/2 + 101.1, which is matched with coordinage (w/2 + 10, h/2 - 50) at image 1 (which both are matched with image 5 respectively).\n",
    "\n",
    "\n",
    "Format:\n",
    "```\n",
    "{\n",
    "  cameraId: { // note cameraId is \"0\", \"1\", ... \"10\"\n",
    "      x: x coordinate of feature away from image center (principal offset),\n",
    "      y: y coordinate of feature away from image center\n",
    "  },\n",
    "  cameraId_2 : {\n",
    "      x: x ...\n",
    "      y: y ...\n",
    "  },\n",
    "  ...\n",
    "  \n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "### File IO\n",
    "#### Reading JSONs\n",
    "```python\n",
    "import json\n",
    "with open('cameras.json', 'r') as f:\n",
    "    camera_json = json.load(f)\n",
    "with open('matches.json', 'r') as f:\n",
    "    matches_json = json.load(f)\n",
    "```\n",
    "#### Camera Object parsing\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "i = 0\n",
    "E = np.array(camera_json[int(i)]['E'])\n",
    "K = np.array(camera_json[int(i)]['K'])\n",
    "# projection matrix\n",
    "P = K.dot(E)\n",
    "```\n",
    "### Visualization\n",
    "#### Camera mesh creation\n",
    "```python\n",
    "def create_camera_mesh(camera_object):\n",
    "    \"\"\"\n",
    "    Takes camera object as input, returns 3x12 camera mesh matrix\n",
    "    \"\"\"\n",
    "    R = np.array(camera_object['R'])\n",
    "    t = np.array(camera_object['t'])\n",
    "    W = np.array(camera_object['W'])\n",
    "    H = np.array(camera_object['H'])\n",
    "    f = np.array(camera_object['K'])[0, 0]\n",
    "    wf = W / f / 2\n",
    "    hf = H / f / 2\n",
    "    CC = -R.dot(t)\n",
    "    o = [0, 0, 0]\n",
    "    lu = [-wf, hf, 1]\n",
    "    ru = [wf, hf, 1]\n",
    "    rd = [wf, -hf, 1]\n",
    "    ld = [-wf, -hf, 1]\n",
    "    corners = np.array([\n",
    "        o, lu, ru, o,\n",
    "        ru, rd, o,\n",
    "        rd, ld, o,\n",
    "        ld, lu\n",
    "    ]).T\n",
    "    CCs = corners + t.reshape(3, 1)\n",
    "    camera_mesh = -R.dot(CCs)\n",
    "    return camera_mesh\n",
    "```\n",
    "#### Visualizing mesh\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib notebook\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.axis('equal')\n",
    "for camera_id, camera_object in camera_json.items():\n",
    "    camera_mesh = create_camera_mesh(camera_object)\n",
    "    ax.plot(camera_mesh[0], camera_mesh[1], zs=camera_mesh[2])\n",
    "```\n",
    "\n",
    "#### Visualizing Points\n",
    "```python\n",
    "test_points = np.load('points.npy')\n",
    "ax.scatter(test_points[0], test_points[1], test_points[2], s=0.05)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
